{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "The core of a machine learning pipeline is to split a complete machine learning task into a multistep workflow. Each step is a manageable component that can be developed, optimized, configured, and automated individually. Steps are connected through well-defined interfaces. The Azure Machine Learning pipeline service automatically orchestrates all the dependencies between pipeline steps. The benefits of using a pipeline are standardized the MLOps practice, scalable team collaboration, training efficiency and cost reduction. To learn more about the benefits of pipelines, see [What are Azure Machine Learning pipelines](https://learn.microsoft.comazure/machine-learning/concept-ml-pipelines).\n",
        "\n",
        "In this tutorial, you use Azure Machine Learning to create a production ready machine learning project, using Azure Machine Learning Python SDK v2.\n",
        "\n",
        "This means you will be able to leverage the AzureML Python SDK to:\n",
        "\n",
        "- Get a handle to your Azure Machine Learning workspace\n",
        "- Create Azure Machine Learning data assets\n",
        "- Create reusable Azure Machine Learning components\n",
        "- Create, validate and run Azure Machine Learning pipelines\n",
        "\n",
        "During this tutorial, you create an Azure Machine Learning pipeline to train a model for credit default prediction. The pipeline handles two steps: \n",
        "\n",
        "1. Data preparation\n",
        "1. Training and registering the trained model\n",
        "\n",
        "The next image shows a simple pipeline as you'll see it in the Azure studio once submitted.\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Set up the pipeline resources\n",
        "\n",
        "The Azure Machine Learning framework can be used from CLI, Python SDK, or studio interface. In this example, you use the Azure Machine Learning Python SDK v2 to create a pipeline. \n",
        "\n",
        "Before creating the pipeline, you need the following resources:\n",
        "\n",
        "* The data asset for training\n",
        "* The software environment to run the pipeline\n",
        "* A compute resource to where the job runs\n",
        "\n",
        "## Create handle to workspace\n",
        "\n",
        "Before we dive in the code, you need a way to reference your workspace. You'll create `ml_client` for a handle to the workspace.  You'll then use `ml_client` to manage resources and jobs.\n",
        "\n",
        "In the next cell, enter your Subscription ID, Resource Group name and Workspace name. To find these values:\n",
        "\n",
        "1. In the upper right Azure Machine Learning studio toolbar, select your workspace name.\n",
        "1. Copy the value for workspace, resource group and subscription ID into the code.\n",
        "1. You'll need to copy one value, close the area and paste, then come back for the next one.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import os\n",
        "\n",
        "# authenticate\n",
        "credential = DefaultAzureCredential()\n",
        "# # Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"6cc13ccc-43a9-461d-abcc-6e482c3e8031\",\n",
        "    resource_group_name=\"cloud-shell-storage-centralindia\",\n",
        "    workspace_name=\"azueml\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "ml_client",
        "gather": {
          "logged": 1685858265497
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "> [!NOTE]\n",
        "> Creating MLClient will not connect to the workspace. The client initialization is lazy, it will wait for the first time it needs to make a call (this will happen when creating the `credit_data` data asset, two code cells from here).\n",
        "\n",
        "## Register data at data asset\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "credit_data = Data(\n",
        "    name=\"creditcard_default\",\n",
        "    path=\"./data/default_card.csv\",\n",
        "    type=AssetTypes.URI_FILE,\n",
        "    description=\"Dataset for credit card defaults\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "name": "credit_data",
        "gather": {
          "logged": 1685858265730
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code just created a `Data` asset, ready to be consumed as an input by the pipeline that you'll define in the next sections. In addition, you can register the data to your workspace so it becomes reusable across pipelines.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "credit_data = ml_client.data.create_or_update(credit_data)\n",
        "print(\n",
        "    f\"Dataset with name {credit_data.name} was registered to workspace, the dataset version is {credit_data.version}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset with name creditcard_default was registered to workspace, the dataset version is 2\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "update-credit_data",
        "gather": {
          "logged": 1685858268659
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Compute Cluster "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure Machine Learning compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure Machine Learning Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"STANDARD_D2_V3\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=4,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "    print(\n",
        "        f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\"\n",
        "    )\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named cpu-cluster, we'll reuse it as is.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "name": "cpu_cluster",
        "gather": {
          "logged": 1685858268861
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a job environment for pipeline steps"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"aml-scikit-learn\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Credit Card Defaults pipeline\",\n",
        "    tags={\"scikit-learn\": \"0.24.2\"},\n",
        "    conda_file=os.path.join(\"dependencies\", \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"0.1.0\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name aml-scikit-learn is registered to workspace, the environment version is 0.1.0\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "custom_env_name",
        "gather": {
          "logged": 1685858270191
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### Create component 1: data prep (using programmatic definition)\n",
        "\n",
        "Let's start by creating the first component. This component handles the preprocessing of the data. The preprocessing task is performed in the *data_prep.py* Python file."
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Now that you have a script that can perform the desired task, create an Azure Machine Learning Component from it.\n",
        "\n",
        "Use the general purpose `CommandComponent` that can run command line actions. This command line action can directly call system commands or run a script. The inputs/outputs are specified on the command line via the `${{ ... }}` notation.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "data_prep_component = command(\n",
        "    name=\"data_prep_credit_defaults\",\n",
        "    display_name=\"Data preparation for training\",\n",
        "    description=\"reads a csv input, split the input to train and test\",\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_folder\"),\n",
        "        \"test_train_ratio\": Input(type=\"number\"),\n",
        "    },\n",
        "    outputs=dict(\n",
        "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    ),\n",
        "    # The source folder of the component\n",
        "    code=\"components/data_prep/\",\n",
        "    command=\"\"\"python data_prep.py \\\n",
        "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
        "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
        "            \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "name": "data_prep_component",
        "gather": {
          "logged": 1685858270420
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, register the component in the workspace for future reuse.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we register the component to the workspace\n",
        "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading data_prep (0.0 MBs):   0%|          | 0/1676 [00:00<?, ?it/s]\r\u001b[32mUploading data_prep (0.0 MBs): 100%|██████████| 1676/1676 [00:00<00:00, 36555.66it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component data_prep_credit_defaults with Version 2023-06-04-06-00-18-0164593 is registered\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1685858272823
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create component 2: training (using yaml definition)\n",
        "\n",
        "The second component that you create consumes the training and test data, train a tree based model and return the output model. Use Azure Machine Learning logging capabilities to record and visualize the learning progress.\n",
        "\n",
        "You used the `CommandComponent` class to create your first component. This time you use the yaml definition to define the second component. Each method has its own advantages. A yaml definition can actually be checked-in along the code, and would provide a readable history tracking. The programmatic method using `CommandComponent` can be easier with built-in class documentation and code completion."
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Now create and register the component.  Registering it allows you to re-use it in other pipelines.  Also, anyone else with access to your workspace can use the registered component."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "train_component = load_component(source=os.path.join(\"components/train\", \"train.yml\"))\n",
        "\n",
        "# Now we register the component to the workspace\n",
        "train_component = ml_client.create_or_update(train_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component train_credit_defaults_model with Version 2023-06-04-06-00-38-6331360 is registered\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "train_component",
        "gather": {
          "logged": 1685858293858
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the pipeline from components\n",
        "\n",
        "Now that both your components are defined and registered, you can start implementing the pipeline.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you use *input data*, *split ratio* and *registered model name* as input variables. Then call the components and connect them via their inputs/outputs identifiers. The outputs of each step can be accessed via the `.outputs` property.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Python functions returned by `load_component()` work as any regular Python function that we use within a pipeline to call each step.\n",
        "\n",
        "To code the pipeline, you use a specific `@dsl.pipeline` decorator that identifies the Azure Machine Learning pipelines. In the decorator, we can specify the pipeline description and default resources like compute and storage. Like a Python function, pipelines can have inputs. You can then create multiple instances of a single pipeline with different inputs.\n",
        "\n",
        "Here, we used *input data*, *split ratio* and *registered model name* as input variables. We then call the components and connect them via their inputs/outputs identifiers. The outputs of each step can be accessed via the `.outputs` property."
      ],
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_target,\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def credit_defaults_pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_learning_rate,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    data_prep_job = data_prep_component(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    train_job = train_component(\n",
        "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
        "        learning_rate=pipeline_job_learning_rate,  # note: using a pipeline input as parameter\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "pipeline",
        "gather": {
          "logged": 1685858297020
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use your pipeline definition to instantiate a pipeline with your dataset, split rate of choice and the name you picked for your model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"credit_defaults_model\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = credit_defaults_pipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path=credit_data.path),\n",
        "    pipeline_job_test_train_ratio=0.25,\n",
        "    pipeline_job_learning_rate=0.05,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        },
        "name": "registered_model_name",
        "gather": {
          "logged": 1685858297303
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit the job \n",
        "\n",
        "It's now time to submit the job to run in Azure Machine Learning. This time you use `create_or_update`  on `ml_client.jobs`.\n",
        "\n",
        "Here you also pass an experiment name. An experiment is a container for all the iterations one does on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in Azure Machine Learning studio.\n",
        "\n",
        "Once completed, the pipeline registers a model in your workspace as a result of training."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    # Project's name\n",
        "    experiment_name=\"pipeline_e2e_compoenents\",\n",
        ")\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: frank_map_y8hvy733yc\nWeb View: https://ml.azure.com/runs/frank_map_y8hvy733yc?wsid=/subscriptions/6cc13ccc-43a9-461d-abcc-6e482c3e8031/resourcegroups/cloud-shell-storage-centralindia/workspaces/azueml\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2023-06-04 06:00:48Z] Submitting 1 runs, first five are: f9d1ac51:00ab5962-b316-42d8-b77b-95a6943694e8\n[2023-06-04 06:05:36Z] Completing processing run id 00ab5962-b316-42d8-b77b-95a6943694e8.\n[2023-06-04 06:05:37Z] Submitting 1 runs, first five are: f8d470b7:93dfaf5a-d850-4e50-8680-c8fbfd6696e3\n[2023-06-04 06:07:50Z] Completing processing run id 93dfaf5a-d850-4e50-8680-c8fbfd6696e3.\n\nExecution Summary\n=================\nRunId: frank_map_y8hvy733yc\nWeb View: https://ml.azure.com/runs/frank_map_y8hvy733yc?wsid=/subscriptions/6cc13ccc-43a9-461d-abcc-6e482c3e8031/resourcegroups/cloud-shell-storage-centralindia/workspaces/azueml\n\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "name": "returned_job",
        "gather": {
          "logged": 1685858743896
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "You can track the progress of your pipeline, by using the link generated in the previous cell. When you first select this link, you may see that the pipeline is still running. Once it's complete, you can examine each component's results.\n",
        "\n",
        "Double-click the **Train Credit Defaults Model** component. \n",
        "\n",
        "There are two important results you'll want to see about training:\n",
        "\n",
        "* View your logs:\n",
        "    1. Select the **Outputs+logs** tab.\n",
        "    1. Open the folders to `user_logs` > `std_log.txt`\n",
        "\n",
        "* View your metrics: Select the **Metrics** tab.  This section shows different logged metrics. In this example. mlflow `autologging`, has automatically logged the training metrics."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "description": {
      "description": "Create production ML pipelines with Python SDK v2 in a Jupyter notebook"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "categories": [
      "SDK v2",
      "tutorials",
      "get-started-notebooks"
    ],
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}